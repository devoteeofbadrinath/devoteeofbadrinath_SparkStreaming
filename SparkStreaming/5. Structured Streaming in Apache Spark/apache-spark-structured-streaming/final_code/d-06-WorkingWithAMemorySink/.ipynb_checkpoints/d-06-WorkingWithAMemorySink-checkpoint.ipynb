{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba14ddb9",
   "metadata": {},
   "source": [
    "Memory sink (for debugging) - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driverâ€™s memory. Hence, use it with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e7f1e",
   "metadata": {},
   "source": [
    "### TODO Recording\n",
    "\n",
    "\n",
    "- Stream data from a file source (`input/` directory)\n",
    "- Check that the `input/` directory exists in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e2b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 19:42:44 WARN Utils: Your hostname, Jananis-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.52 instead (on interface en0)\n",
      "25/02/12 19:42:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/12 19:42:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DoubleType, LongType)\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MemorySinkDemo\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e20584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming DataFrame created. Monitoring 'input/' directory for new files...\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Manufacturer\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Segment\", StringType(), True),\n",
    "    StructField(\"Total_Cores\", LongType(), True),\n",
    "    StructField(\"Processor_Speed\", IntegerType(), True),\n",
    "    StructField(\"CoProcessor_Cores\", StringType(), True), \n",
    "    StructField(\"Rmax\", DoubleType(), True),\n",
    "    StructField(\"Rpeak\", DoubleType(), True),\n",
    "    StructField(\"Power\", DoubleType(), True),\n",
    "    StructField(\"Power_Efficiency\", DoubleType(), True),\n",
    "    StructField(\"Architecture\", StringType(), True),\n",
    "    StructField(\"Processor_Technology\", StringType(), True),\n",
    "    StructField(\"Operating_System\", StringType(), True),\n",
    "    StructField(\"OS_Family\", StringType(), True),\n",
    "])\n",
    "\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"input/\")\n",
    "\n",
    "print(\"Streaming DataFrame created. Monitoring 'input/' directory for new files...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e49f0",
   "metadata": {},
   "source": [
    "Apply Transformations to the Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f7b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations applied to streaming DataFrame.\n"
     ]
    }
   ],
   "source": [
    "transformed_df = streaming_df \\\n",
    "    .select(\"Rank\", \"Name\", \"Country\", \"Processor_Speed\", \"Rmax\") \\\n",
    "    .filter(col(\"Country\") == \"Japan\") \\\n",
    "    .withColumn(\"Processing_Time\", current_timestamp())\n",
    "\n",
    "print(\"Transformations applied to streaming DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941a66e",
   "metadata": {},
   "source": [
    "Write the Transformed Data to a Memory Sink and Query the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d2c64",
   "metadata": {},
   "source": [
    "Adaptive Query Execution (AQE) (spark.sql.adaptive.enabled) is an optimization feature in batch processing.\n",
    "However, AQE is not supported in Streaming DataFrames, so Spark disables it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b33638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 19:43:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = transformed_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"supercomputer_memory_sink\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/\")\\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dfbb0b",
   "metadata": {},
   "source": [
    "### TODO Recording:\n",
    "\n",
    "- Add the JSON files to the input/ folder\n",
    "- After each file is added come back to the notebook and run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09cd34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------+-------+---------------+--------+-----------------------+\n",
      "|Rank|Name                              |Country|Processor_Speed|Rmax    |Processing_Time        |\n",
      "+----+----------------------------------+-------+---------------+--------+-----------------------+\n",
      "|1   |Supercomputer Fugaku              |Japan  |2200           |442010.0|2025-02-12 19:43:02.247|\n",
      "|16  |ABCI 2.0                          |Japan  |2400           |22208.72|2025-02-12 19:43:02.247|\n",
      "|17  |Wisteria/BDEC-01 (Odyssey)        |Japan  |2200           |22121.0 |2025-02-12 19:43:02.247|\n",
      "|31  |TOKI-SORA                         |Japan  |2200           |16592.0 |2025-02-12 19:43:02.247|\n",
      "|39  |Oakforest-PACS                    |Japan  |1400           |13554.6 |2025-02-12 19:43:02.247|\n",
      "|48  |Earth Simulator -SX-Aurora TSUBASA|Japan  |1600           |9990.7  |2025-02-12 19:43:02.247|\n",
      "|59  |TSUBAME3.0                        |Japan  |2400           |8125.0  |2025-02-12 19:43:02.247|\n",
      "|63  |Plasma Simulator                  |Japan  |1584           |7892.7  |2025-02-12 19:43:02.247|\n",
      "|73  |Flow                              |Japan  |2200           |6617.8  |2025-02-12 19:43:02.247|\n",
      "|78  |NULL                              |Japan  |3000           |6162.0  |2025-02-12 19:43:02.247|\n",
      "|79  |SQUID - CPU Nodes                 |Japan  |2400           |6105.1  |2025-02-12 19:43:02.247|\n",
      "|86  |NULL                              |Japan  |2100           |5730.5  |2025-02-12 19:43:02.247|\n",
      "|87  |NULL                              |Japan  |2100           |5730.5  |2025-02-12 19:43:02.247|\n",
      "|98  |Flow Type II subsystem            |Japan  |2100           |4880.46 |2025-02-12 19:43:16.256|\n",
      "|103 |ITO - Subsystem A                 |Japan  |3000           |4540.69 |2025-02-12 19:43:16.256|\n",
      "|106 |Wisteria/BDEC-01 (Aquarius)       |Japan  |2400           |4425.0  |2025-02-12 19:43:16.256|\n",
      "|110 |Oakbridge-CX                      |Japan  |2700           |4289.85 |2025-02-12 19:43:16.256|\n",
      "|112 |NULL                              |Japan  |2300           |4128.0  |2025-02-12 19:43:16.256|\n",
      "|122 |NULL                              |Japan  |2700           |3712.0  |2025-02-12 19:43:16.256|\n",
      "|134 |Ohtaka                            |Japan  |2000           |3486.1  |2025-02-12 19:43:25.526|\n",
      "|160 |Numerical Materials Simulator     |Japan  |2900           |3082.12 |2025-02-12 19:43:25.526|\n",
      "|162 |Camphor 2                         |Japan  |1400           |3057.35 |2025-02-12 19:43:37.717|\n",
      "|190 |JFRS-1                            |Japan  |2400           |2787.14 |2025-02-12 19:43:37.717|\n",
      "+----+----------------------------------+-------+---------------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\"SELECT * FROM supercomputer_memory_sink\")\n",
    "\n",
    "result_df.show(n=50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e594ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d1ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
