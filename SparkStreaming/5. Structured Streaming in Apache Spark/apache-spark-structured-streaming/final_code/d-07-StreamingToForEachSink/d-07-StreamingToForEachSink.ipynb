{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8f2b5c",
   "metadata": {},
   "source": [
    "### TODO Recording:\n",
    "\n",
    "- Set up the input/ folder, make sure it is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e138ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 20:23:13 WARN Utils: Your hostname, Jananis-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.52 instead (on interface en0)\n",
      "25/02/12 20:23:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/12 20:23:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/12 20:23:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FileStreamForeachPrintDemo\").getOrCreate()\n",
    "print('Spark session created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c1013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined successfully.\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Manufacturer\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Segment\", StringType(), True),\n",
    "    StructField(\"Total_Cores\", IntegerType(), True),\n",
    "    StructField(\"Processor_Speed\", IntegerType(), True),\n",
    "    StructField(\"CoProcessor_Cores\", IntegerType(), True),\n",
    "    StructField(\"Rmax\", DoubleType(), True),\n",
    "    StructField(\"Rpeak\", DoubleType(), True),\n",
    "    StructField(\"Power\", DoubleType(), True),\n",
    "    StructField(\"Power_Efficiency\", DoubleType(), True),\n",
    "    StructField(\"Architecture\", StringType(), True),\n",
    "    StructField(\"Processor_Technology\", StringType(), True),\n",
    "    StructField(\"Operating_System\", StringType(), True),\n",
    "    StructField(\"OS_Family\", StringType(), True)\n",
    "])\n",
    "\n",
    "print('Schema defined successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8261bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"input/\")\n",
    "\n",
    "transformed_df = streaming_df \\\n",
    "    .select(\"Rank\", \"Name\", \"Country\", \"Processor_Speed\", \"Rmax\") \\\n",
    "    .filter(\"Country = 'Japan'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355eb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    name = row.Name if row.Name is not None else \"Unknown\"\n",
    "    country = row.Country if row.Country is not None else \"Unknown\"\n",
    "    speed = row.Processor_Speed if row.Processor_Speed is not None else \"Unknown\"\n",
    "    rmax = row.Rmax if row.Rmax is not None else \"Unknown\"\n",
    "\n",
    "    print(f\"Processing row: Name={name}, Country={country}, Speed={speed}, Rmax={rmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca18e1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 20:23:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/f4/wmvdntf96vjfrq3sb2p4ncy00000gn/T/temporary-cbca3c40-4829-462d-98f4-4ea4cc909155. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/02/12 20:23:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Processing row: Name=Supercomputer Fugaku, Country=Japan, Speed=2200, Rmax=442010.0\n",
      "Processing row: Name=ABCI 2.0, Country=Japan, Speed=2400, Rmax=22208.72\n",
      "Processing row: Name=Wisteria/BDEC-01 (Odyssey), Country=Japan, Speed=2200, Rmax=22121.0\n",
      "Processing row: Name=TOKI-SORA, Country=Japan, Speed=2200, Rmax=16592.0\n",
      "Processing row: Name=Oakforest-PACS, Country=Japan, Speed=1400, Rmax=13554.6\n",
      "Processing row: Name=Earth Simulator -SX-Aurora TSUBASA, Country=Japan, Speed=1600, Rmax=9990.7\n",
      "Processing row: Name=TSUBAME3.0, Country=Japan, Speed=2400, Rmax=8125.0         \n",
      "Processing row: Name=Plasma Simulator, Country=Japan, Speed=1584, Rmax=7892.7\n",
      "Processing row: Name=Flow, Country=Japan, Speed=2200, Rmax=6617.8\n",
      "Processing row: Name=Unknown, Country=Japan, Speed=3000, Rmax=6162.0\n",
      "Processing row: Name=SQUID - CPU Nodes, Country=Japan, Speed=2400, Rmax=6105.1\n",
      "Processing row: Name=Unknown, Country=Japan, Speed=2100, Rmax=5730.5\n",
      "Processing row: Name=Unknown, Country=Japan, Speed=2100, Rmax=5730.5\n"
     ]
    }
   ],
   "source": [
    "query = transformed_df.writeStream \\\n",
    "            .foreach(process_row) \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ddc22c",
   "metadata": {},
   "source": [
    "### TODO Recording:\n",
    "\n",
    "- Add 1 file to the input/ folder\n",
    "- Show the result\n",
    "- Add another file to the input folder\n",
    "- Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ff4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0befe6",
   "metadata": {},
   "source": [
    "### TODO Recording:\n",
    "\n",
    "- Clean out the input/ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f912c5c",
   "metadata": {},
   "source": [
    "We create a custom class `PrintRowWriter` to process each row of the streaming data. This class implements three methods:\n",
    "\n",
    "- **open()**: Called once per partition per epoch.\n",
    "- **process()**: Called for each row. Here, we print the row details.\n",
    "- **close()**: Called when the processing for the partition ends (to handle any errors).\n",
    "\n",
    "This custom writer is used in the `foreach` sink.\n",
    "\n",
    "What are partitionId and epochId?\n",
    "When using a custom sink (like PrintRowWriter), Spark Structured Streaming calls the open() method for each partition in every micro-batch.\n",
    "\n",
    "- partitionId\n",
    "Represents the partition number within the current micro-batch.\n",
    "If a DataFrame has multiple partitions, open() is called separately for each partition.\n",
    "Partitioning depends on the number of executors and parallelism settings.\n",
    "- epochId\n",
    "Represents the unique micro-batch ID in Spark Structured Streaming.\n",
    "Used to ensure exactly-once processing (important for fault tolerance).\n",
    "If Spark retries a micro-batch, it reuses the same epochId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f9aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"input/\")\n",
    "\n",
    "transformed_df = streaming_df \\\n",
    "    .select(\"Rank\", \"Name\", \"Country\", \"Processor_Speed\", \"Rmax\") \\\n",
    "    .filter(\"Country = 'Japan'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07e2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintRowWriter:\n",
    "    def open(self, partitionId, epochId):\n",
    "        print(f\"Opened partition {partitionId} for epoch {epochId}\")\n",
    "        return True  # Return True to indicate that processing should continue\n",
    "\n",
    "    def process(self, row):\n",
    "        try:\n",
    "            if row is None:\n",
    "                print(\"Skipping null row\")\n",
    "                return\n",
    "\n",
    "            name = row.Name if row.Name is not None else \"Unknown\"\n",
    "            country = row.Country \\\n",
    "                if row.Country is not None else \"Unknown\"\n",
    "            speed = row.Processor_Speed \\\n",
    "                if row.Processor_Speed is not None else \"Unknown\"\n",
    "            rmax = row.Rmax if row.Rmax is not None else \"Unknown\"\n",
    "            performance_score = (speed * rmax) / 10**6\n",
    "\n",
    "            print(f\"Processing row: Name={name}, Country={country}, \"\n",
    "                  f\"Performance Score={performance_score}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing row: {e}\")\n",
    "\n",
    "    def close(self, error):\n",
    "        if error:\n",
    "            print(f\"❌ Encountered error in partition: {error}\")\n",
    "        else:\n",
    "            print(\"✔ Partition processing completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3acfa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 20:24:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/f4/wmvdntf96vjfrq3sb2p4ncy00000gn/T/temporary-fa0694cc-a7db-4b41-a8a5-c9c2455644ca. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/02/12 20:24:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Opened partition 0 for epoch 0\n",
      "Processing row: Name=Supercomputer Fugaku, Country=Japan, Performance Score=972.422\n",
      "Processing row: Name=ABCI 2.0, Country=Japan, Performance Score=53.300928\n",
      "Processing row: Name=Wisteria/BDEC-01 (Odyssey), Country=Japan, Performance Score=48.6662\n",
      "Processing row: Name=TOKI-SORA, Country=Japan, Performance Score=36.5024\n",
      "Processing row: Name=Oakforest-PACS, Country=Japan, Performance Score=18.97644\n",
      "Processing row: Name=Earth Simulator -SX-Aurora TSUBASA, Country=Japan, Performance Score=15.985120000000002\n",
      "✔ Partition processing completed successfully.\n",
      "Opened partition 0 for epoch 1\n",
      "Processing row: Name=TSUBAME3.0, Country=Japan, Performance Score=19.5\n",
      "Processing row: Name=Plasma Simulator, Country=Japan, Performance Score=12.502036799999999\n",
      "Processing row: Name=Flow, Country=Japan, Performance Score=14.55916\n",
      "Processing row: Name=Unknown, Country=Japan, Performance Score=18.486\n",
      "Processing row: Name=SQUID - CPU Nodes, Country=Japan, Performance Score=14.65224\n",
      "Processing row: Name=Unknown, Country=Japan, Performance Score=12.03405\n",
      "Processing row: Name=Unknown, Country=Japan, Performance Score=12.03405\n",
      "✔ Partition processing completed successfully.\n",
      "Opened partition 1 for epoch 2\n",
      "Processing row: Name=Ohtaka, Country=Japan, Performance Score=6.9722\n",
      "Processing row: Name=Numerical Materials Simulator, Country=Japan, Performance Score=8.938148\n",
      "✔ Partition processing completed successfully.\n",
      "Opened partition 0 for epoch 2\n",
      "Processing row: Name=Flow Type II subsystem, Country=Japan, Performance Score=10.248966\n",
      "Processing row: Name=ITO - Subsystem A, Country=Japan, Performance Score=13.622069999999999\n",
      "Processing row: Name=Wisteria/BDEC-01 (Aquarius), Country=Japan, Performance Score=10.62\n",
      "Processing row: Name=Oakbridge-CX, Country=Japan, Performance Score=11.582595000000001\n",
      "Processing row: Name=Unknown, Country=Japan, Performance Score=9.4944\n",
      "Processing row: Name=Unknown, Country=Japan, Performance Score=10.0224\n",
      "✔ Partition processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "query = transformed_df.writeStream \\\n",
    "            .foreach(PrintRowWriter()) \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b57e8c",
   "metadata": {},
   "source": [
    "### TODO Recording:\n",
    "\n",
    "- After running the previous cell place 1 file in the input/\n",
    "- Show the output printed to screen (will be partition 0 and epoch 0)\n",
    "- Add another file (will be partition 0 and epoch 1)\n",
    "- Add 2 files in one go (will be partition 0/1 and epoch 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac2b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d599798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
